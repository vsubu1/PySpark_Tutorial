{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLUNpj0ypzMO3bHy85yFpH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vsubu1/PySpark_Tutorial/blob/main/PySparkTraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHFZlQlRTLEw",
        "outputId": "1ab1c5ea-0e64-426d-e7c0-9ba82254cb16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+-------+\n",
            "| Name|Age|Country|\n",
            "+-----+---+-------+\n",
            "| John| 25|    USA|\n",
            "| Mike| 35|  Japan|\n",
            "|Peter| 32|    USA|\n",
            "+-----+---+-------+\n",
            "\n",
            "+-----+---+\n",
            "| Name|Age|\n",
            "+-----+---+\n",
            "| John| 25|\n",
            "| Mike| 35|\n",
            "|Peter| 32|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### 1. **Select specific columns from a DataFrame**\n",
        "###   Write a PySpark transformation to select only the columns `col1`, `col2`, and `col3` from a DataFrame.\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"Select Columns\").getOrCreate()\n",
        "\n",
        "data = [(\"John\",25, \"USA\"), (\"Mike\",35, \"Japan\"),(\"Peter\",32,\"USA\")]\n",
        "columns = [\"Name\",\"Age\",\"Country\"]\n",
        "df = spark.createDataFrame(data,columns)\n",
        "df.show()\n",
        "\n",
        "selected_df = df.select(\"Name\",\"Age\")\n",
        "selected_df.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 2. Write a PySpark transformation to rename column `old_name` to `new_name` in a DataFrame.\n",
        "df_renamed = df.withColumnRenamed(\"Name\",\"New Name\")\n",
        "df_renamed.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYuzOyrMcgue",
        "outputId": "a71df660-1cbf-4678-e34e-18385c126fde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---+-------+\n",
            "|New Name|Age|Country|\n",
            "+--------+---+-------+\n",
            "|    John| 25|    USA|\n",
            "|    Mike| 35|  Japan|\n",
            "|   Peter| 32|    USA|\n",
            "+--------+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 3.  Add a new column 'new_column' with the constant value 100\n",
        "df_newColumn = df.withColumn(\"Salary\",lit(1000))\n",
        "df_newColumn.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXN7BbYXcwYj",
        "outputId": "de204bea-b91e-42e3-b244-eb5cbff075dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+-------+------+\n",
            "| Name|Age|Country|Salary|\n",
            "+-----+---+-------+------+\n",
            "| John| 25|    USA|  1000|\n",
            "| Mike| 35|  Japan|  1000|\n",
            "|Peter| 32|    USA|  1000|\n",
            "+-----+---+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 4. Drop column\n",
        "df_dropColumn = df_newColumn.drop(\"Salary\")\n",
        "df_dropColumn.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SE_FgFrSdOPf",
        "outputId": "adb85576-b057-464f-cc33-6f2fd03793ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+-------+\n",
            "| Name|Age|Country|\n",
            "+-----+---+-------+\n",
            "| John| 25|    USA|\n",
            "| Mike| 35|  Japan|\n",
            "|Peter| 32|    USA|\n",
            "+-----+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 5. **Perform an inner join on two DataFrames**\n",
        "###   Write a PySpark transformation to perform an inner join between two DataFrames `df1` and `df2` based on the common column `id`.\n",
        "\n",
        "spark= SparkSession.builder.master(\"local\").appName(\"Inner join\").getOrCreate()\n",
        "\n",
        "data1 = [(1,\"John\",\"5000\"), (2,\"Peter\",2000), (3,\"Mike\",3000),(1,\"Britto\",5000),(2,\"Alpha\",6000),(3,\"Stanley\",3000),(4,\"Subu\",1000)]\n",
        "columns1 = [\"id\",\"name\",\"salary\"]\n",
        "\n",
        "data2 = [(1,\"Science\"),(2,\"Maths\"),(3,\"Engineering\"),(5,\"Medicine\")]\n",
        "columns2 = [\"id\",\"Department\"]\n",
        "\n",
        "df1 = spark.createDataFrame(data1,columns1)\n",
        "df2 = spark.createDataFrame(data2,columns2)\n",
        "\n",
        "idf = df1.join(df2,on=\"id\",how=\"inner\")\n",
        "idf.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2N3rOpGdoei",
        "outputId": "d67d5505-cdfe-4377-d59b-4c35d97f9295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+-----------+\n",
            "| id|   name|salary| Department|\n",
            "+---+-------+------+-----------+\n",
            "|  1|   John|  5000|    Science|\n",
            "|  1| Britto|  5000|    Science|\n",
            "|  2|  Peter|  2000|      Maths|\n",
            "|  2|  Alpha|  6000|      Maths|\n",
            "|  3|   Mike|  3000|Engineering|\n",
            "|  3|Stanley|  3000|Engineering|\n",
            "+---+-------+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 6. **Group by a column and calculate the average**\n",
        "###   Write a PySpark transformation that groups a DataFrame by the column `department` and calculates the average salary for each department.\n",
        "\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "idf.groupBy(\"Department\").agg(avg(\"salary\").alias(\"Avg Salary\")).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHH-0ERLfOWu",
        "outputId": "d2720da5-3cfc-4738-d7d1-99cd1549d283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+\n",
            "| Department|Avg Salary|\n",
            "+-----------+----------+\n",
            "|    Science|    5000.0|\n",
            "|Engineering|    3000.0|\n",
            "|      Maths|    4000.0|\n",
            "+-----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 7. Write a PySpark transformation that returns distinct values from the `Department` column.\n",
        "\n",
        "idf.select(\"id\",\"department\").distinct().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v145wOShGh6",
        "outputId": "9d02dc5c-9341-4c20-bd7b-513e569048bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------+\n",
            "| id| department|\n",
            "+---+-----------+\n",
            "|  1|    Science|\n",
            "|  2|      Maths|\n",
            "|  3|Engineering|\n",
            "+---+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 8. **Apply a UDF to a column**\n",
        "###  Write a PySpark transformation that applies a user-defined function (UDF) to the `salary` column to increase salary by $3000\n",
        "\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "def hikeSalary(salary) :\n",
        "  return(salary+3000)\n",
        "\n",
        "# Register the UDF with the appropriate return type (IntegerType)\n",
        "hikeSalaryUDF = udf(hikeSalary,IntegerType())\n",
        "\n",
        "# Cast the 'salary' column to IntegerType before applying the UDF\n",
        "hdf = idf.withColumn(\"newSalary\",hikeSalary(col(\"salary\").cast(IntegerType())))\n",
        "hdf.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tT09YVXRhe7l",
        "outputId": "43a977da-2898-46eb-a5e6-5d75f8d791d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+-----------+---------+\n",
            "| id|   name|salary| Department|newSalary|\n",
            "+---+-------+------+-----------+---------+\n",
            "|  1|   John|  5000|    Science|     8000|\n",
            "|  1| Britto|  5000|    Science|     8000|\n",
            "|  2|  Peter|  2000|      Maths|     5000|\n",
            "|  2|  Alpha|  6000|      Maths|     9000|\n",
            "|  3|   Mike|  3000|Engineering|     6000|\n",
            "|  3|Stanley|  3000|Engineering|     6000|\n",
            "+---+-------+------+-----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 9. Write a PySpark transformation that calculates the length of the strings in the `name` column and adds the result as a new column `name_length`.\n",
        "\n",
        "from pyspark.sql.functions import length\n",
        "ldf = hdf.withColumn(\"name_length\",length(col(\"name\")))\n",
        "ldf.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPtNGiCRle6k",
        "outputId": "aef3cb8a-72f8-4dfa-a702-525308b73cd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+-----------+---------+-----------+\n",
            "| id|   name|salary| Department|newSalary|name_length|\n",
            "+---+-------+------+-----------+---------+-----------+\n",
            "|  1|   John|  5000|    Science|     8000|          4|\n",
            "|  1| Britto|  5000|    Science|     8000|          6|\n",
            "|  2|  Peter|  2000|      Maths|     5000|          5|\n",
            "|  2|  Alpha|  6000|      Maths|     9000|          5|\n",
            "|  3|   Mike|  3000|Engineering|     6000|          4|\n",
            "|  3|Stanley|  3000|Engineering|     6000|          7|\n",
            "+---+-------+------+-----------+---------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 10. Write a PySpark transformation that creates a new column `status`, which is `True` if the value in the `salary` column is greater than 5000, else `False`.\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "cdf = ldf.withColumn(\"status\",when(col(\"newSalary\") > 5000,True).otherwise(False))\n",
        "cdf.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6vxpTDFoEsG",
        "outputId": "0ec43db2-aeab-45f3-8382-2c49d5666e3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+-----------+---------+-----------+------+\n",
            "| id|   name|salary| Department|newSalary|name_length|status|\n",
            "+---+-------+------+-----------+---------+-----------+------+\n",
            "|  1|   John|  5000|    Science|     8000|          4|  true|\n",
            "|  1| Britto|  5000|    Science|     8000|          6|  true|\n",
            "|  2|  Peter|  2000|      Maths|     5000|          5| false|\n",
            "|  2|  Alpha|  6000|      Maths|     9000|          5|  true|\n",
            "|  3|   Mike|  3000|Engineering|     6000|          4|  true|\n",
            "|  3|Stanley|  3000|Engineering|     6000|          7|  true|\n",
            "+---+-------+------+-----------+---------+-----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 11. **Find the maximum value in a column**\n",
        "###  Write a PySpark transformation to find the maximum value in the `salary` column.\n",
        "\n",
        "from pyspark.sql.functions import max\n",
        "cdf.agg(max(\"salary\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZnS-CF8oqQy",
        "outputId": "03f7f0ce-e013-4ee6-e22a-e322b480eb1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|max(salary)|\n",
            "+-----------+\n",
            "|       6000|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 12. **Perform a left outer, Right Outer and Inner joins**\n",
        "### Write a PySpark transformation to perform a left outer join between two DataFrames `df1` and `df2` on the `id` column.\n",
        "\n",
        "data1 = [(1,\"John\",2000),(2,\"Peter\",3000),(3,\"Mike\",4000),(4,\"Subu\",5000)]\n",
        "columns1 = [\"id\",\"name\",\"salary\"]\n",
        "\n",
        "data2 = [(1,\"Science\"),(2,\"Maths\"),(3,\"Engineering\"),(5,\"Medicine\")]\n",
        "columns2 = [\"id\",\"Department\"]\n",
        "\n",
        "df1 = spark.createDataFrame(data1,columns1)\n",
        "df2 = spark.createDataFrame(data2,columns2)\n",
        "\n",
        "ldf = df1.join(df2,on=\"id\",how=\"left\")\n",
        "ldf.show()\n",
        "\n",
        "rdf = df1.join(df2,on=\"id\",how=\"right\")\n",
        "rdf.show()\n",
        "\n",
        "idf = df1.join(df2,on=\"id\",how=\"inner\")\n",
        "idf.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVKJNtb-qKmx",
        "outputId": "5e97cc67-04e2-44d7-c6c4-e9d6b5cb3e0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+------+-----------+\n",
            "| id| name|salary| Department|\n",
            "+---+-----+------+-----------+\n",
            "|  1| John|  2000|    Science|\n",
            "|  3| Mike|  4000|Engineering|\n",
            "|  2|Peter|  3000|      Maths|\n",
            "|  4| Subu|  5000|       NULL|\n",
            "+---+-----+------+-----------+\n",
            "\n",
            "+---+-----+------+-----------+\n",
            "| id| name|salary| Department|\n",
            "+---+-----+------+-----------+\n",
            "|  5| NULL|  NULL|   Medicine|\n",
            "|  1| John|  2000|    Science|\n",
            "|  3| Mike|  4000|Engineering|\n",
            "|  2|Peter|  3000|      Maths|\n",
            "+---+-----+------+-----------+\n",
            "\n",
            "+---+-----+------+-----------+\n",
            "| id| name|salary| Department|\n",
            "+---+-----+------+-----------+\n",
            "|  1| John|  2000|    Science|\n",
            "|  2|Peter|  3000|      Maths|\n",
            "|  3| Mike|  4000|Engineering|\n",
            "+---+-----+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 13 Write a pyspark code to find second highest salary from a data frame using windowing functions\n",
        "\n",
        "from pyspark.sql.functions import row_number, col, desc, asc\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "windowspec = Window.orderBy(col(\"Salary\").desc())\n",
        "wdf = idf.withColumn(\"RowId\",row_number().over(windowspec))\n",
        "wdf.show()\n",
        "\n",
        "wdf.filter(col(\"RowId\") == 2).show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nb03epvorxky",
        "outputId": "4bae1f0e-5144-4b1a-e7ae-15e8d2fca8cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+------+-----------+-----+\n",
            "| id| name|salary| Department|RowId|\n",
            "+---+-----+------+-----------+-----+\n",
            "|  3| Mike|  4000|Engineering|    1|\n",
            "|  2|Peter|  3000|      Maths|    2|\n",
            "|  1| John|  2000|    Science|    3|\n",
            "+---+-----+------+-----------+-----+\n",
            "\n",
            "+---+-----+------+----------+-----+\n",
            "| id| name|salary|Department|RowId|\n",
            "+---+-----+------+----------+-----+\n",
            "|  2|Peter|  3000|     Maths|    2|\n",
            "+---+-----+------+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 14 Write a pyspark code to find second highest salary from each department using windowing functions\n",
        "\n",
        "data1 = [(1,\"John\",\"5000\"), (2,\"Peter\",2000), (3,\"Mike\",4000),(1,\"Britto\",6000),(2,\"Alpha\",6000),(3,\"Stanley\",3000),(4,\"Subu\",1000)]\n",
        "columns1 = [\"id\",\"name\",\"salary\"]\n",
        "\n",
        "data2 = [(1,\"Science\"),(2,\"Maths\"),(3,\"Engineering\"),(5,\"Medicine\")]\n",
        "columns2 = [\"id\",\"Department\"]\n",
        "\n",
        "df1 = spark.createDataFrame(data1,columns1)\n",
        "\n",
        "windowspec = Window.partitionBy(\"id\").orderBy(col(\"Salary\").desc())\n",
        "wdf = df1.withColumn(\"RowNumber\",row_number().over(windowspec))\n",
        "wdf.show()\n",
        "\n",
        "wdf.filter(col(\"RowNumber\")==2).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fg4CsNf6tbep",
        "outputId": "85da7a44-8f2e-4096-eff9-36a3d18789d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+---------+\n",
            "| id|   name|salary|RowNumber|\n",
            "+---+-------+------+---------+\n",
            "|  1| Britto|  6000|        1|\n",
            "|  1|   John|  5000|        2|\n",
            "|  2|  Alpha|  6000|        1|\n",
            "|  2|  Peter|  2000|        2|\n",
            "|  3|   Mike|  4000|        1|\n",
            "|  3|Stanley|  3000|        2|\n",
            "|  4|   Subu|  1000|        1|\n",
            "+---+-------+------+---------+\n",
            "\n",
            "+---+-------+------+---------+\n",
            "| id|   name|salary|RowNumber|\n",
            "+---+-------+------+---------+\n",
            "|  1|   John|  5000|        2|\n",
            "|  2|  Peter|  2000|        2|\n",
            "|  3|Stanley|  3000|        2|\n",
            "+---+-------+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 15. **Drop rows with missing values**\n",
        "### Write a PySpark transformation that removes rows with `null` values in any column.\n",
        "\n",
        "data1 = [(1,\"John\",2000),(2,\"Peter\",3000),(3,\"Mike\",4000),(4,\"Subu\",5000)]\n",
        "columns1 = [\"id\",\"name\",\"salary\"]\n",
        "\n",
        "data2 = [(1,\"Science\"),(2,\"Maths\"),(3,\"Engineering\"),(5,\"Medicine\")]\n",
        "columns2 = [\"id\",\"Department\"]\n",
        "\n",
        "df1 = spark.createDataFrame(data1,columns1)\n",
        "df2 = spark.createDataFrame(data2,columns2)\n",
        "\n",
        "ldf = df1.join(df2,on=\"id\",how=\"left\")\n",
        "ldf.show()\n",
        "\n",
        "rdf = df1.join(df2,on=\"id\",how=\"right\")\n",
        "rdf.show()\n",
        "\n",
        "idf = df1.join(df2,on=\"id\",how=\"inner\")\n",
        "idf.show()\n",
        "\n",
        "ldf.na.drop().show()\n",
        "rdf.na.drop().show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBLyOvXav1Rq",
        "outputId": "fb52a150-7cb0-4953-d807-092ab53514b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+------+-----------+\n",
            "| id| name|salary| Department|\n",
            "+---+-----+------+-----------+\n",
            "|  1| John|  2000|    Science|\n",
            "|  3| Mike|  4000|Engineering|\n",
            "|  2|Peter|  3000|      Maths|\n",
            "|  4| Subu|  5000|       NULL|\n",
            "+---+-----+------+-----------+\n",
            "\n",
            "+---+-----+------+-----------+\n",
            "| id| name|salary| Department|\n",
            "+---+-----+------+-----------+\n",
            "|  5| NULL|  NULL|   Medicine|\n",
            "|  1| John|  2000|    Science|\n",
            "|  3| Mike|  4000|Engineering|\n",
            "|  2|Peter|  3000|      Maths|\n",
            "+---+-----+------+-----------+\n",
            "\n",
            "+---+-----+------+-----------+\n",
            "| id| name|salary| Department|\n",
            "+---+-----+------+-----------+\n",
            "|  1| John|  2000|    Science|\n",
            "|  2|Peter|  3000|      Maths|\n",
            "|  3| Mike|  4000|Engineering|\n",
            "+---+-----+------+-----------+\n",
            "\n",
            "+---+-----+------+-----------+\n",
            "| id| name|salary| Department|\n",
            "+---+-----+------+-----------+\n",
            "|  1| John|  2000|    Science|\n",
            "|  3| Mike|  4000|Engineering|\n",
            "|  2|Peter|  3000|      Maths|\n",
            "+---+-----+------+-----------+\n",
            "\n",
            "+---+-----+------+-----------+\n",
            "| id| name|salary| Department|\n",
            "+---+-----+------+-----------+\n",
            "|  1| John|  2000|    Science|\n",
            "|  3| Mike|  4000|Engineering|\n",
            "|  2|Peter|  3000|      Maths|\n",
            "+---+-----+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 16. **Convert a column to lowercase**\n",
        "### Write a PySpark transformation that converts all the values in the column `name` to lowercase.\n",
        "\n",
        "from pyspark.sql.functions import lower\n",
        "\n",
        "xdf = ldf.withColumn(\"name\",lower(col(\"name\"))).withColumn(\"Department\",lower(col(\"Department\")))\n",
        "xdf.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gU4WeI9AwkpY",
        "outputId": "dfadc306-299a-48bd-b1c2-c1980e719e68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+------+-----------+\n",
            "| id| name|salary| Department|\n",
            "+---+-----+------+-----------+\n",
            "|  1| john|  2000|    science|\n",
            "|  3| mike|  4000|engineering|\n",
            "|  2|peter|  3000|      maths|\n",
            "|  4| subu|  5000|       NULL|\n",
            "+---+-----+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 17. **Extract a substring from a column**\n",
        "### Write a PySpark transformation that extracts the first 3 characters from the `name` column.\n",
        "\n",
        "from pyspark.sql.functions import substring\n",
        "\n",
        "xdf.withColumn(\"name\",substring(col(\"name\"),1,3)).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgXNDcvixSXm",
        "outputId": "7ac5910e-d21f-4e23-ad1f-55fdf68c769c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+------+-----------+\n",
            "| id|name|salary| Department|\n",
            "+---+----+------+-----------+\n",
            "|  1| joh|  2000|    science|\n",
            "|  3| mik|  4000|engineering|\n",
            "|  2| pet|  3000|      maths|\n",
            "|  4| sub|  5000|       NULL|\n",
            "+---+----+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 17. **Extract a replace  column values**\n",
        "### Write a PySpark transformation that replaces NULL with a \"Not Available\"\n",
        "\n",
        "xdf.na.fill(\"Not Available\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GZCqo1HxiTD",
        "outputId": "f7ba3d8f-e0f0-4f04-d3ab-00b0ce4f1dcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+------+-------------+\n",
            "| id| name|salary|   Department|\n",
            "+---+-----+------+-------------+\n",
            "|  1| john|  2000|      science|\n",
            "|  3| mike|  4000|  engineering|\n",
            "|  2|peter|  3000|        maths|\n",
            "|  4| subu|  5000|Not Available|\n",
            "+---+-----+------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6edb6b22",
        "outputId": "71c4df6d-bec7-4a9f-ecbb-246b765e4464"
      },
      "source": [
        "### 18. Write a PySpark transformation that uses explode function\n",
        "\n",
        "from pyspark.sql.functions import explode\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"Explode Example\").getOrCreate()\n",
        "\n",
        "data = [(\"John\", [\"Java\", \"Python\"]), (\"Mike\", [\"C++\", \"Scala\", \"R\"]), (\"Peter\", [\"Spark\"])]\n",
        "columns = [\"Name\", \"Languages\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "\n",
        "exploded_df = df.select(\"Name\", explode(\"Languages\").alias(\"Language\"))\n",
        "exploded_df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------------+\n",
            "| Name|      Languages|\n",
            "+-----+---------------+\n",
            "| John| [Java, Python]|\n",
            "| Mike|[C++, Scala, R]|\n",
            "|Peter|        [Spark]|\n",
            "+-----+---------------+\n",
            "\n",
            "+-----+--------+\n",
            "| Name|Language|\n",
            "+-----+--------+\n",
            "| John|    Java|\n",
            "| John|  Python|\n",
            "| Mike|     C++|\n",
            "| Mike|   Scala|\n",
            "| Mike|       R|\n",
            "|Peter|   Spark|\n",
            "+-----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 19. Write a PySpark code to explain all windowing functions with  a simple example each\n",
        "\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number, rank, dense_rank, ntile, lag, lead, percent_rank, cume_dist, col, avg, sum, min, max\n",
        "\n",
        "# Create a sample DataFrame for windowing examples\n",
        "data_window = [\n",
        "    (\"Sales\", \"John\", 5000),\n",
        "    (\"Sales\", \"Peter\", 6000),\n",
        "    (\"Sales\", \"Mike\", 5500),\n",
        "    (\"IT\", \"Alpha\", 7000),\n",
        "    (\"IT\", \"Britto\", 6500),\n",
        "    (\"IT\", \"Stanley\", 7200),\n",
        "    (\"HR\", \"Subu\", 4500),\n",
        "    (\"HR\", \"Mary\", 4800)\n",
        "]\n",
        "columns_window = [\"Department\", \"Name\", \"Salary\"]\n",
        "df_window = spark.createDataFrame(data_window, columns_window)\n",
        "df_window.show()\n",
        "\n",
        "# Define a window specification partitioned by Department and ordered by Salary in descending order\n",
        "window_spec_salary_desc = Window.partitionBy(\"Department\").orderBy(col(\"Salary\").desc())\n",
        "\n",
        "# 1. row_number()\n",
        "# Assigns a sequential rank to each row within its partition, starting from 1.\n",
        "df_window_rownumber = df_window.withColumn(\"row_number\", row_number().over(window_spec_salary_desc))\n",
        "print(\"row_number(): Assigns a sequential rank to each row within its partition.\")\n",
        "df_window_rownumber.show()\n",
        "\n",
        "# 2. rank()\n",
        "# Assigns a rank to each row within its partition. Rows with the same value receive the same rank, and there are gaps in the sequence if ranks are skipped.\n",
        "df_window_rank = df_window.withColumn(\"rank\", rank().over(window_spec_salary_desc))\n",
        "print(\"rank(): Assigns a rank with gaps for ties.\")\n",
        "df_window_rank.show()\n",
        "\n",
        "# 3. dense_rank()\n",
        "# Assigns a rank to each row within its partition. Rows with the same value receive the same rank, and there are no gaps in the sequence.\n",
        "df_window_dense_rank = df_window.withColumn(\"dense_rank\", dense_rank().over(window_spec_salary_desc))\n",
        "print(\"dense_rank(): Assigns a rank without gaps for ties.\")\n",
        "df_window_dense_rank.show()\n",
        "\n",
        "# 4. ntile(n)\n",
        "# Divides the rows within a partition into n approximately equal groups (buckets) and assigns a bucket number (from 1 to n) to each row.\n",
        "# Let's divide each department into 2 groups based on salary\n",
        "df_window_ntile = df_window.withColumn(\"ntile_2\", ntile(2).over(window_spec_salary_desc))\n",
        "print(\"ntile(n): Divides rows into n groups.\")\n",
        "df_window_ntile.show()\n",
        "\n",
        "# Define a window specification ordered by Salary ascending for lag and lead\n",
        "window_spec_salary_asc = Window.partitionBy(\"Department\").orderBy(col(\"Salary\").asc())\n",
        "\n",
        "# 5. lag(column, offset, default)\n",
        "# Returns the value of an expression from a preceding row within the partition.\n",
        "# offset specifies how many rows back to look (default is 1).\n",
        "# default specifies the value to return when the offset goes beyond the partition boundaries.\n",
        "df_window_lag = df_window.withColumn(\"previous_salary\", lag(\"Salary\", 1, 0).over(window_spec_salary_asc))\n",
        "print(\"lag(): Value from a preceding row.\")\n",
        "df_window_lag.show()\n",
        "\n",
        "# 6. lead(column, offset, default)\n",
        "# Returns the value of an expression from a subsequent row within the partition.\n",
        "# offset specifies how many rows forward to look (default is 1).\n",
        "# default specifies the value to return when the offset goes beyond the partition boundaries.\n",
        "df_window_lead = df_window.withColumn(\"next_salary\", lead(\"Salary\", 1, 0).over(window_spec_salary_asc))\n",
        "print(\"lead(): Value from a subsequent row.\")\n",
        "df_window_lead.show()\n",
        "\n",
        "# 7. percent_rank()\n",
        "# Calculates the percentile rank of a row within its partition. It is (rank - 1) / (total_rows - 1).\n",
        "df_window_percent_rank = df_window.withColumn(\"percent_rank\", percent_rank().over(window_spec_salary_desc))\n",
        "print(\"percent_rank(): Percentile rank of a row.\")\n",
        "df_window_percent_rank.show()\n",
        "\n",
        "# 8. cume_dist()\n",
        "# Calculates the cumulative distribution of a value within its partition. It is the number of rows less than or equal to the current row divided by the total number of rows in the partition.\n",
        "df_window_cume_dist = df_window.withColumn(\"cume_dist\", cume_dist().over(window_spec_salary_desc))\n",
        "print(\"cume_dist(): Cumulative distribution of a value.\")\n",
        "df_window_cume_dist.show()\n",
        "\n",
        "# Aggregate Window Functions\n",
        "# These functions perform aggregations (like sum, avg, min, max, count) over a window of rows.\n",
        "# We can define different window frames for these.\n",
        "\n",
        "# Define a window specification partitioned by Department (no ordering needed for some aggregates, but important for others like moving average)\n",
        "window_spec_department = Window.partitionBy(\"Department\")\n",
        "\n",
        "# 9. avg()\n",
        "# Calculates the average of a column within the window.\n",
        "df_window_avg = df_window.withColumn(\"avg_dept_salary\", avg(\"Salary\").over(window_spec_department))\n",
        "print(\"avg(): Average salary within the department.\")\n",
        "df_window_avg.show()\n",
        "\n",
        "# 10. sum()\n",
        "# Calculates the sum of a column within the window.\n",
        "df_window_sum = df_window.withColumn(\"total_dept_salary\", sum(\"Salary\").over(window_spec_department))\n",
        "print(\"sum(): Total salary within the department.\")\n",
        "df_window_sum.show()\n",
        "\n",
        "# 11. min()\n",
        "# Finds the minimum value of a column within the window.\n",
        "df_window_min = df_window.withColumn(\"min_dept_salary\", min(\"Salary\").over(window_spec_department))\n",
        "print(\"min(): Minimum salary within the department.\")\n",
        "df_window_min.show()\n",
        "\n",
        "# 12. max()\n",
        "# Finds the maximum value of a column within the window.\n",
        "df_window_max = df_window.withColumn(\"max_dept_salary\", max(\"Salary\").over(window_spec_department))\n",
        "print(\"max(): Maximum salary within the department.\")\n",
        "df_window_max.show()\n",
        "\n",
        "# Example of a moving average using a window frame\n",
        "# Calculate the average salary of the current row and the preceding row within each department, ordered by salary.\n",
        "window_spec_moving_avg = Window.partitionBy(\"Department\").orderBy(\"Salary\").rowsBetween(-1, 0)\n",
        "df_window_moving_avg = df_window.withColumn(\"moving_avg_salary\", avg(\"Salary\").over(window_spec_moving_avg))\n",
        "print(\"avg() with moving window: Average salary of current and previous row.\")\n",
        "df_window_moving_avg.show()\n",
        "\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dujrkqfWzOYy",
        "outputId": "ca54663c-6fe3-41fa-d0d1-c22818fcc6ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+------+\n",
            "|Department|   Name|Salary|\n",
            "+----------+-------+------+\n",
            "|     Sales|   John|  5000|\n",
            "|     Sales|  Peter|  6000|\n",
            "|     Sales|   Mike|  5500|\n",
            "|        IT|  Alpha|  7000|\n",
            "|        IT| Britto|  6500|\n",
            "|        IT|Stanley|  7200|\n",
            "|        HR|   Subu|  4500|\n",
            "|        HR|   Mary|  4800|\n",
            "+----------+-------+------+\n",
            "\n",
            "row_number(): Assigns a sequential rank to each row within its partition.\n",
            "+----------+-------+------+----------+\n",
            "|Department|   Name|Salary|row_number|\n",
            "+----------+-------+------+----------+\n",
            "|        HR|   Mary|  4800|         1|\n",
            "|        HR|   Subu|  4500|         2|\n",
            "|        IT|Stanley|  7200|         1|\n",
            "|        IT|  Alpha|  7000|         2|\n",
            "|        IT| Britto|  6500|         3|\n",
            "|     Sales|  Peter|  6000|         1|\n",
            "|     Sales|   Mike|  5500|         2|\n",
            "|     Sales|   John|  5000|         3|\n",
            "+----------+-------+------+----------+\n",
            "\n",
            "rank(): Assigns a rank with gaps for ties.\n",
            "+----------+-------+------+----+\n",
            "|Department|   Name|Salary|rank|\n",
            "+----------+-------+------+----+\n",
            "|        HR|   Mary|  4800|   1|\n",
            "|        HR|   Subu|  4500|   2|\n",
            "|        IT|Stanley|  7200|   1|\n",
            "|        IT|  Alpha|  7000|   2|\n",
            "|        IT| Britto|  6500|   3|\n",
            "|     Sales|  Peter|  6000|   1|\n",
            "|     Sales|   Mike|  5500|   2|\n",
            "|     Sales|   John|  5000|   3|\n",
            "+----------+-------+------+----+\n",
            "\n",
            "dense_rank(): Assigns a rank without gaps for ties.\n",
            "+----------+-------+------+----------+\n",
            "|Department|   Name|Salary|dense_rank|\n",
            "+----------+-------+------+----------+\n",
            "|        HR|   Mary|  4800|         1|\n",
            "|        HR|   Subu|  4500|         2|\n",
            "|        IT|Stanley|  7200|         1|\n",
            "|        IT|  Alpha|  7000|         2|\n",
            "|        IT| Britto|  6500|         3|\n",
            "|     Sales|  Peter|  6000|         1|\n",
            "|     Sales|   Mike|  5500|         2|\n",
            "|     Sales|   John|  5000|         3|\n",
            "+----------+-------+------+----------+\n",
            "\n",
            "ntile(n): Divides rows into n groups.\n",
            "+----------+-------+------+-------+\n",
            "|Department|   Name|Salary|ntile_2|\n",
            "+----------+-------+------+-------+\n",
            "|        HR|   Mary|  4800|      1|\n",
            "|        HR|   Subu|  4500|      2|\n",
            "|        IT|Stanley|  7200|      1|\n",
            "|        IT|  Alpha|  7000|      1|\n",
            "|        IT| Britto|  6500|      2|\n",
            "|     Sales|  Peter|  6000|      1|\n",
            "|     Sales|   Mike|  5500|      1|\n",
            "|     Sales|   John|  5000|      2|\n",
            "+----------+-------+------+-------+\n",
            "\n",
            "lag(): Value from a preceding row.\n",
            "+----------+-------+------+---------------+\n",
            "|Department|   Name|Salary|previous_salary|\n",
            "+----------+-------+------+---------------+\n",
            "|        HR|   Subu|  4500|              0|\n",
            "|        HR|   Mary|  4800|           4500|\n",
            "|        IT| Britto|  6500|              0|\n",
            "|        IT|  Alpha|  7000|           6500|\n",
            "|        IT|Stanley|  7200|           7000|\n",
            "|     Sales|   John|  5000|              0|\n",
            "|     Sales|   Mike|  5500|           5000|\n",
            "|     Sales|  Peter|  6000|           5500|\n",
            "+----------+-------+------+---------------+\n",
            "\n",
            "lead(): Value from a subsequent row.\n",
            "+----------+-------+------+-----------+\n",
            "|Department|   Name|Salary|next_salary|\n",
            "+----------+-------+------+-----------+\n",
            "|        HR|   Subu|  4500|       4800|\n",
            "|        HR|   Mary|  4800|          0|\n",
            "|        IT| Britto|  6500|       7000|\n",
            "|        IT|  Alpha|  7000|       7200|\n",
            "|        IT|Stanley|  7200|          0|\n",
            "|     Sales|   John|  5000|       5500|\n",
            "|     Sales|   Mike|  5500|       6000|\n",
            "|     Sales|  Peter|  6000|          0|\n",
            "+----------+-------+------+-----------+\n",
            "\n",
            "percent_rank(): Percentile rank of a row.\n",
            "+----------+-------+------+------------+\n",
            "|Department|   Name|Salary|percent_rank|\n",
            "+----------+-------+------+------------+\n",
            "|        HR|   Mary|  4800|         0.0|\n",
            "|        HR|   Subu|  4500|         1.0|\n",
            "|        IT|Stanley|  7200|         0.0|\n",
            "|        IT|  Alpha|  7000|         0.5|\n",
            "|        IT| Britto|  6500|         1.0|\n",
            "|     Sales|  Peter|  6000|         0.0|\n",
            "|     Sales|   Mike|  5500|         0.5|\n",
            "|     Sales|   John|  5000|         1.0|\n",
            "+----------+-------+------+------------+\n",
            "\n",
            "cume_dist(): Cumulative distribution of a value.\n",
            "+----------+-------+------+------------------+\n",
            "|Department|   Name|Salary|         cume_dist|\n",
            "+----------+-------+------+------------------+\n",
            "|        HR|   Mary|  4800|               0.5|\n",
            "|        HR|   Subu|  4500|               1.0|\n",
            "|        IT|Stanley|  7200|0.3333333333333333|\n",
            "|        IT|  Alpha|  7000|0.6666666666666666|\n",
            "|        IT| Britto|  6500|               1.0|\n",
            "|     Sales|  Peter|  6000|0.3333333333333333|\n",
            "|     Sales|   Mike|  5500|0.6666666666666666|\n",
            "|     Sales|   John|  5000|               1.0|\n",
            "+----------+-------+------+------------------+\n",
            "\n",
            "avg(): Average salary within the department.\n",
            "+----------+-------+------+---------------+\n",
            "|Department|   Name|Salary|avg_dept_salary|\n",
            "+----------+-------+------+---------------+\n",
            "|        HR|   Subu|  4500|         4650.0|\n",
            "|        HR|   Mary|  4800|         4650.0|\n",
            "|        IT|  Alpha|  7000|         6900.0|\n",
            "|        IT| Britto|  6500|         6900.0|\n",
            "|        IT|Stanley|  7200|         6900.0|\n",
            "|     Sales|   John|  5000|         5500.0|\n",
            "|     Sales|  Peter|  6000|         5500.0|\n",
            "|     Sales|   Mike|  5500|         5500.0|\n",
            "+----------+-------+------+---------------+\n",
            "\n",
            "sum(): Total salary within the department.\n",
            "+----------+-------+------+-----------------+\n",
            "|Department|   Name|Salary|total_dept_salary|\n",
            "+----------+-------+------+-----------------+\n",
            "|        HR|   Subu|  4500|             9300|\n",
            "|        HR|   Mary|  4800|             9300|\n",
            "|        IT|  Alpha|  7000|            20700|\n",
            "|        IT| Britto|  6500|            20700|\n",
            "|        IT|Stanley|  7200|            20700|\n",
            "|     Sales|   John|  5000|            16500|\n",
            "|     Sales|  Peter|  6000|            16500|\n",
            "|     Sales|   Mike|  5500|            16500|\n",
            "+----------+-------+------+-----------------+\n",
            "\n",
            "min(): Minimum salary within the department.\n",
            "+----------+-------+------+---------------+\n",
            "|Department|   Name|Salary|min_dept_salary|\n",
            "+----------+-------+------+---------------+\n",
            "|        HR|   Subu|  4500|           4500|\n",
            "|        HR|   Mary|  4800|           4500|\n",
            "|        IT|  Alpha|  7000|           6500|\n",
            "|        IT| Britto|  6500|           6500|\n",
            "|        IT|Stanley|  7200|           6500|\n",
            "|     Sales|   John|  5000|           5000|\n",
            "|     Sales|  Peter|  6000|           5000|\n",
            "|     Sales|   Mike|  5500|           5000|\n",
            "+----------+-------+------+---------------+\n",
            "\n",
            "max(): Maximum salary within the department.\n",
            "+----------+-------+------+---------------+\n",
            "|Department|   Name|Salary|max_dept_salary|\n",
            "+----------+-------+------+---------------+\n",
            "|        HR|   Subu|  4500|           4800|\n",
            "|        HR|   Mary|  4800|           4800|\n",
            "|        IT|  Alpha|  7000|           7200|\n",
            "|        IT| Britto|  6500|           7200|\n",
            "|        IT|Stanley|  7200|           7200|\n",
            "|     Sales|   John|  5000|           6000|\n",
            "|     Sales|  Peter|  6000|           6000|\n",
            "|     Sales|   Mike|  5500|           6000|\n",
            "+----------+-------+------+---------------+\n",
            "\n",
            "avg() with moving window: Average salary of current and previous row.\n",
            "+----------+-------+------+-----------------+\n",
            "|Department|   Name|Salary|moving_avg_salary|\n",
            "+----------+-------+------+-----------------+\n",
            "|        HR|   Subu|  4500|           4500.0|\n",
            "|        HR|   Mary|  4800|           4650.0|\n",
            "|        IT| Britto|  6500|           6500.0|\n",
            "|        IT|  Alpha|  7000|           6750.0|\n",
            "|        IT|Stanley|  7200|           7100.0|\n",
            "|     Sales|   John|  5000|           5000.0|\n",
            "|     Sales|   Mike|  5500|           5250.0|\n",
            "|     Sales|  Peter|  6000|           5750.0|\n",
            "+----------+-------+------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l7psoAG_1EUV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}